<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<script src="https://kit.fontawesome.com/8c6b6fca0e.js" crossorigin="anonymous"></script>

<style type="text/css">
	body {
		font-family: "Tahoma", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
		font-weight: 300;
		font-size: 16px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
		text-align: center;

	}

	h1 {
		font-weight: 300;
		margin-left: auto;
		margin-right: auto;
		text-align: center;

	}

	h2 {
		text-align: center;
		font-weight: normal;
		margin-left: auto;
		margin-right: auto;
	}

	p {
		width: 1000px;
		margin-left: auto;
		margin-right: auto;
		color:#222222;
	}

	sup{
		color:#555555;
	}
	.disclaimerbox {
		background-color: #eee;
		border: 1px solid #eeeeee;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
	}

	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
	}

	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
	}

	a:link,
	a:visited {
		color: #1367a7;
		text-decoration: none;
	}

	a:hover {
		color: #208799;
	}

	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}


	hr {
		border: 0;
		height: 2px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}

	.author_image {
		text-align: center;
	}
</style>

<html>

<head>
	<title>Uncertainty-Aware Deep Multi-View Photometric Stereo</title>
	<meta property="og:image" content="" />
	<meta property="og:title" content="Uncertainty-Aware Deep Multi-View Photometric Stereo"/>
	<link href='https://fonts.googleapis.com/css?family=Lora:400italic' rel='stylesheet' type='text/css'>
</head>

<!--Main page starts here-->
<body>

	<!-- Title -->
	<br>
	<br>
	<center>
		<span style="font-size:36px; color:#333333">Uncertainty-Aware Deep Multi-View Photometric Stereo</span>
	</center>
	<br>



	<!-- Authors -->
	<table align=center width=1000px>
		<tr>
			<td align=center width=100px>
				<center>
					<span style="font-size:18px"><a href="https://scholar.google.com/citations?user=Lgz3NgkAAAAJ&hl=en">Berk Kaya</a><sup style="font-size:10px">1</sup></span>
				</center>
			</td>	
			
			<td align=center width=100px>
				<center>
					<span style="font-size:18px"><a href="https://suryanshkumar.github.io/">Suryansh Kumar</a><sup style="font-size:10px">1</sup></span>
				</center>
			</td>
			
			<td align=center width=100px>
				<center>
					<span style="font-size:18px"><a href="https://ee.ethz.ch/the-department/people-a-z/person-detail.MjM1NjA3.TGlzdC8zMjc5LC0xNjUwNTg5ODIw.html">Carlos Oliveira</a><sup style="font-size:10px">1</sup></span>
				</center>
			</td>

			<td align=center width=100px>
				<center>
					<span style="font-size:18px"><a href="https://sites.google.com/view/vittoferrari">Vittorio Ferrari</a><sup style="font-size:10px">2</sup></span>
				</center>
			</td>

			<td align=center width=100px>
				<center>
					<span style="font-size:18px"><a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en">Luc Van Gool</a><sup style="font-size:10px">1,3</sup></span>
				</center>
			</td>
		</tr>
	</table>



    <!-- Affiliations -->
	<table align=center width=700px>
		<tr>
			<td align=center width=100px>
				<center>
					<span style="font-size:18px">ETH Zurich<sup style="font-size:10px">1</sup>,  &nbsp Google Research<sup style="font-size:10px">2</sup>,  &nbsp KU Lueven</span><sup style="font-size:10px">3</sup></span>
				</center>
			</td>
		</tr>
	</table>
	<br>


	<!-- Conference -->
	<center>
		<span style="font-size:26px; color:#333333; font-family: Helvetica; font-weight: 500;">CVPR 2022</span>
	</center>
	<br>
	<br>



	<!-- Links -->
	<table align=center width=600>
		<tr>
			<td width=100px>
				<center>
                <a href="./images/uncertainty_aware_deep_multiview_photometric_stereo.pdf">
                    <i class="fa fa-file-pdf fa-2xl"></i> 
                </a> <br>
                </center>
			</td>
			<td width=100px>
				<center>
                <a href="./images/supplementary.pdf">
                    <i class="fa fa-file-pdf fa-2xl"></i> 
                </a> <br>
                </center>
			</td>
			<td width=100px>
				<center>
					<a href="https://github.com/berk95kaya/UA-MVPS">
                    <i class="fa fa-github fa-2xl"></i>
                </a> <br>	
				</center>
			</td>
			<!--<td width=100px>
				<center>
                <a href="https://paschalidoud.github.io/data/Paschalidou2021NEURIPS_poster.pdf">
                    <i class="fa fa-picture-o"></i> <br/>
                    Poster
                </a>
				<center>
			</td>

			<td width=100px>
				<center>
	                <a href="https://paschalidoud.github.io/data/Paschalidou2021NEURIPS_slides.pdf">
                    <i class="fa fa-file-powerpoint-o"></i> <br/>
                    Slides
				<center>
			</td>-->
		</tr>

		<tr>
			<td>
				<center>
  					<span> <br> Paper</span>
                </center>
			</td>
						<td>
				<center>
  					 <span> <br> Supplementary</span>

                </center>
			</td>
						<td>
				<center>
					<span> <br> Code (Coming Soon)</span>
                </center>
			</td>
	</table>
	<br>
<br>
	<!-- Image for the project-->
	<table align=center width=1000>
		<tr>
			<td width=400px>
				<center>
					<a><img src="./images/mvs.png" height="150px"></img></href></a><br>
				</center>
			</td>
			<td width=400px>
				<center>
					<a><img src="./images/ps.png" height="150px"></img></href></a>
				</center>
			</td>			
			<td width=400px>
				<center>
					<a><img src="./images/fusion.png" height="150px"></img></href></a>
				</center>
			</td>
		</tr>
		<tr>
			<td width=400px>
				<center>					
					<span style="font-size:15px; font-family:Thoma ; font-weight: 500;">Multi-View Stereo Reconstruction</span>
				<center>
			</td>
			<td width=400px>
				<center>
					<span style="font-size:15px; font-family:Thoma ; font-weight: 500;">Photometric Stereo Reconstruction</span>
				<center>
			</td>
			<td width=400px>
				<center>
					<span style="font-size:15px; font-family:Thoma ; font-weight: 500;">Our Approach <br></span>
				<center>
			</td>




		</tr>
	</table>
	<br>


	<hr>

	<!-- Abstract of the project-->
	<h2 > Abstract </h2>
	<p style="text-align: justify";>
		<font style="font-family: Calibri  ; font-size: 18px; font-weight: 500">
		This paper presents a simple and effective solution to the longstanding classical multi-view photometric stereo (MVPS) problem. It is well-known that photometric stereo (PS) is excellent at recovering high-frequency surface details, whereas multi-view stereo (MVS) can help remove the low-frequency distortion due to PS and retain the global geometry of the shape. This paper proposes an approach that can effectively utilize such complementary strengths of PS and MVS. Our key idea is to combine them suitably while considering the per-pixel uncertainty of their estimates. To this end, we estimate per-pixel surface normals and depth using an uncertainty-aware deep-PS network and deep-MVS network, respectively. Uncertainty modeling helps select reliable surface normal and depth estimates at each pixel which then act as a true representative of the dense surface geometry. At each pixel, our approach either selects or discards deep-PS and deep-MVS network prediction depending on the prediction uncertainty measure. For dense, detailed, and precise inference of the object's surface profile, we propose to learn the implicit neural shape representation via a multilayer perceptron (MLP). Our approach encourages the MLP to converge to a natural zero-level set surface using the confident prediction from deep-PS and deep-MVS networks, providing superior dense surface reconstruction. Extensive experiments on the DiLiGenT-MV benchmark dataset show that our method provides high-quality shape recovery with a much lower memory footprint while outperforming almost all of the existing approaches.
		</font>
	</p><br>
	
		<a><img src="./images/pipeline.png" width="1000px"></img></href></a><br>
		<br>

	<hr>

	<!--Some Results-->
	
	<center>
		<h1>Results</h1>
	</center>
	<table align=center width=1100px>
		<tr>
			<td width=400px>
				<center> <span align="top" style="font-size:16px"> Light Directions and Intensities on DiLiGenT Benchmark</span><br><br> </center>
			</td>
			<td width=50px> </td>
			<td width=400px>
				<center> <span align="top" style="font-size:16px">Surface Normals on DiLiGenT Benchmark</span><br><br> </center>
			</td>
		</tr>

		<tr height="300px">
			<td valign="top" width=400px>
				<center>
					<a><img src="./images/light.png" height="250px"></img></href></a>
				</center>
			</td>

			<td width=50px> </td>
			<td valign="top" width=400px>
				<center>
					<a><img src="./images/normal.png" height="250px"></img></href></a>
				</center>
			</td>
		</tr>
	</table>
	<br>
	<hr> 

	<!--Code or github link-->
<!-- 	<center>
		<h1>Code and Dataset</h1>
	</center>
	<table align=center width=1000px>
		<tr>
			<center>
				<br><span style="font-size:28px">&nbsp;<a
						href="https://github.com/suryanshkumar">[GitHub]</a></span><span style="font-size:28px"></span>
					<span style="font-size:28px">&nbsp;<a
						href="https://data.vision.ee.ethz.ch/bekaya/PS_Datasets/Dome_Dataset.zip">[Dataset]</a></span><span style="font-size:28px"></span>
				<br>
			</center>
		</tr>
	</table>
	<br>
	<hr> -->

	<!--Results (Pictures or Youtube link) or Presentation link-->
	<a name="results"></a>


	<center>
		<h1>Video Presentation (The audio works well on Chrome, Safari)</h1>
	</center><br><br>
	<table align=center width=1100px>
		<!--Video link-->
		<tr height="400px">
			<td valign="top" width=1100px>
				<center>
					<video width="1000" controls autoplay muted loop> 
						<source src="./videos/wacv2022_presentation_video.mp4" type="video/mp4">
						Your browser does not support the video tag.
					</video>
				</center>
			</td>
		</tr>
	</table>
	<br>
	<hr>
	
 	<!--Authors-->
	<h2>Authors</h2>

	<table align=center width=1000px>
		<tr>
			<td>
				<div class="author_image"><img style="height:100px" src="./images/authors/berk.jpg"><p>Berk Kaya</p></div>
			</td>
			<td>
				<div class="author_image"><img style="height:100px" src="./images/authors/suryansh.jpg"><p>Suryansh Kumar</p></div>
			</td>

			<td>
				<div class="author_image"><img style="height:100px" src="./images/authors/cadu.jpg"><p>Carlos Oliveira</p></div>
			</td>
			<td>
				<div class="author_image"><img style="height:100px" src="./images/authors/vitto.png"><p>Vittorio Ferrari</p></div>
			</td>
			<td>
				<div class="author_image"><img style="height:100px" src="./images/authors/luc.png">
				<p>Luc Van Gool</p></div>	
			</td>	
		</tr>
	</table>
	<hr>

	<h2> Citation </h2>
    <p style="width:600px; text-align: left";><code >
	@article{kaya2022uncertainty,
	title={Uncertainty-Aware Deep Multi-View Photometric Stereo},
	author={Kaya, Berk and Kumar, Suryansh and Oliveira,
	Carlos and Ferrari, Vittorio and Van Gool, Luc},
	journal={CVPR}, year={2022}}}
	</code></pre>
	


	<!--Acknowledgements-->
	<table align=center width=1100px>
		<center>
			<h1>Acknowledgements</h1>
		</center>
		<tr>
			<td>
				<img style="height:50px" src="./images/eth_zurich_logo.png">
			</td>
			<td>
				<img style="height:50px" src="./images/cvl.png">
			</td>
			<td>
				<img style="height:50px" src="./images/google_logo.png">
			</td>

			<td>
				<p style="text-align: justify;">
					This work was funded by Focused Research Award from Google(CVL, ETH 2019-HE-318, 2019-HE-323). Suryansh Kumar's project is supported by "ETH Zurich Foundation and Google, Project Number: 2019-HE-323" for bringing together best academic and industrial research.
				</p>
			</td>
		</tr>
	</table>
	<br><br>

</body>

</html>
